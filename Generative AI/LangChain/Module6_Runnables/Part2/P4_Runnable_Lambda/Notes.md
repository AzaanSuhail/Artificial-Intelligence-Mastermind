# ğŸ§  What is `RunnableLambda`?

`RunnableLambda` is a **wrapper that turns any Python function (lambda or normal function)** into a LangChain **Runnable** â€” making it compatible with LCEL pipelines like `RunnableSequence`, `RunnableParallel`, etc.

In simple terms:

> It allows you to inject custom Python logic into a LangChain pipeline.

---

## âš™ï¸ Conceptual Overview

* A **Runnable** in LangChain is any object that implements `.invoke()`, `.batch()`, and `.stream()`.
* `RunnableLambda` lets you quickly **wrap a function** that transforms data without writing a new Runnable class.

You can think of it like this:

```python
RunnableLambda(fn)(input) â†’ fn(input)

```

---

## ğŸ§© Basic Example

```python
from langchain_core.runnables import RunnableLambda

# Define a simple lambda function
reverse_text = RunnableLambda(lambda x: x[::-1])

# Run it
output = reverse_text.invoke("LangChain")
print(output)

```

ğŸŸ¢ **Output:**

```python
niahCgnaL

```

âœ… The input `"LangChain"` was reversed â€” the lambda function runs inside LangChainâ€™s runnable framework.

---

## ğŸ§± Why Itâ€™s Important

`RunnableLambda` is extremely useful for:

* Adding **custom transformations** (formatting, parsing, pre/post-processing)
* **Debugging** pipelines (print/log data mid-pipeline)
* Performing **conditional routing**
* **Bridging** between LangChain and your own Python logic

---

## ğŸ§© Example â€” Custom Transformation in a Sequence

Letâ€™s say you want to process text, summarize it, and then uppercase the result.

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda, RunnableSequence

# Define runnables
prompt = PromptTemplate.from_template("Summarize this text:\\n{text}")
llm = ChatOpenAI(model="gpt-4o-mini")

# Custom lambda to uppercase result
uppercase = RunnableLambda(lambda x: x.upper())

# Build the chain
chain = RunnableSequence(first=prompt | llm, last=uppercase)

output = chain.invoke({"text": "LangChain enables composable AI pipelines."})
print(output)

```

ğŸŸ© **Output Example:**

```
LANGCHAIN IS A TOOL FOR BUILDING AI WORKFLOWS.

```

---

## ğŸ§© Example â€” Conditional Logic

You can also use Python logic to branch behavior dynamically:

```python
def conditional_logic(x):
    if "error" in x.lower():
        return "âš ï¸ Error detected!"
    return f"âœ… Processed: {x}"

logic_runnable = RunnableLambda(conditional_logic)

print(logic_runnable.invoke("System running fine"))
print(logic_runnable.invoke("Error: API limit reached"))

```

ğŸŸ¢ **Output:**

```
âœ… Processed: System running fine
âš ï¸ Error detected!

```

---

## ğŸ§  How It Fits in the LCEL Ecosystem

| Runnable                | Purpose                                              |
| ----------------------- | ---------------------------------------------------- |
| `RunnableSequence`    | Connects runnables in a sequential flow              |
| `RunnableParallel`    | Runs multiple runnables simultaneously               |
| `RunnablePassthrough` | Passes data unchanged                                |
| âœ…`RunnableLambda`    | Executes a Python function or lambda within the flow |

Thus, `RunnableLambda` is the **bridge** between LangChainâ€™s declarative pipelines and your  **custom Python logic** .

---

## ğŸ§© Example â€” In a Complex Pipeline

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda

split_words = RunnableLambda(lambda x: x["text"].split())
count_words = RunnableLambda(lambda words: len(words))

chain = RunnableParallel({
    "original": RunnablePassthrough(),
    "word_count": split_words | count_words
})

result = chain.invoke({"text": "LangChain makes LLM orchestration easy"})
print(result)

```

ğŸŸ¢ **Output:**

```python
{
  "original": {"text": "LangChain makes LLM orchestration easy"},
  "word_count": 5
}

```

---

# ğŸ§­ Summary Table

| Feature             | Description                      | Example Use                   |
| ------------------- | -------------------------------- | ----------------------------- |
| Type                | Utility Runnable                 | Wraps Python functions        |
| Input/Output        | Any type                         | Dict, str, list, etc.         |
| Usage               | Pre/post-processing              | Cleaning, formatting, routing |
| Chain Compatibility | âœ… Works with all LCEL pipelines | Sequence, Parallel, Map       |

---

# ğŸ“š Authoritative Learning & Reading Resources

Here are **high-quality, official, and expert-level sources** you should follow for deeper understanding:

1. **RunnableLambda API Reference (Official LangChain Docs)**
   ğŸ”— [https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)
2. **LangChain Expression Language (LCEL) Concepts**
   ğŸ”— [](https://python.langchain.com/docs/concepts/lcel?utm_source=chatgpt.com)[https://python.langchain.com/docs/concepts/lcel](https://python.langchain.com/docs/concepts/lcel)
3. **LangChain How-To: Sequence Composition**
   ğŸ”— [](https://python.langchain.com/docs/how_to/sequence?utm_source=chatgpt.com)[https://python.langchain.com/docs/how_to/sequence](https://python.langchain.com/docs/how_to/sequence)
4. **Advanced Tutorial: Runnables & LCEL Patterns (Medium)**
   ğŸ”— [https://medium.com/@mishra.sagar25/langchain-series-part-8-mastering-runnables-and-lcel-9e1273aeed7a](https://medium.com/@mishra.sagar25/langchain-series-part-8-mastering-runnables-and-lcel-9e1273aeed7a)
5. **Mirascope Blog â€“ Understanding Runnables**
   ğŸ”— [](https://mirascope.com/blog/langchain-runnables?utm_source=chatgpt.com)[https://mirascope.com/blog/langchain-runnables](https://mirascope.com/blog/langchain-runnables)
