## **Conditional (Branching) Chains in LangChain / GenAI Workflows**

---

### ğŸ§  1. Intuitive Understanding

A *conditional chain* (also sometimes called a  *branching chain* ) allows your pipeline to **make decisions** â€” i.e., â€œIf X then do Chain A, else do Chain Bâ€.

This is more advanced than a straight sequential chain. For example:

* If the user sentiment is positive â†’ respond with â€œThanks for the feedback!â€ chain
* If the sentiment is negative â†’ respond with â€œHow can we improve?â€ chain
  Thus, you add logic / branching into your GenAI workflow.

---

### âš™ï¸ 2. Definition

> A Conditional Chain is a workflow component that evaluates a condition (based on input or intermediate output) and routes execution to one of multiple sub-chains depending on which condition is satisfied.

Key features:

* **Input** arrives â†’ some decision logic (function or LLM) runs
* Based on result â†’ one of several sub-chains is executed
* Output comes from whichever branch was chosen
* You can also have a *default branch* if none of the conditions match

---

### ğŸª„ 3. Visual Intuition

```
             [Input]
                â†“
         [Decision Logic]
          /            \\
   Condition A true   Condition A false
       â†“                     â†“
  [Chain A]             [Chain B / Default]
       â†“                     â†“
    [Output]             [Output]

```

You can think of it like a â€œforkâ€ in your pipeline where the road splits based on a check.

---

### ğŸ’» 4. Example: Conditional Chain in Python

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema import RunnableBranch
from langchain.chat_models import ChatOpenAI

# 1) Setup LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 2) Define chains
template_positive = PromptTemplate(
    input_variables=["review"],
    template="Respond to this positive review in a friendly way:\\n\\n{review}"
)
positive_chain = LLMChain(llm=llm, prompt=template_positive)

template_negative = PromptTemplate(
    input_variables=["review"],
    template="Respond to this negative review with empathy and ask how we can improve:\\n\\n{review}"
)
negative_chain = LLMChain(llm=llm, prompt=template_negative)

# 3) Decision function
def is_positive(input_dict: dict) -> bool:
    # simple heuristic: check for â€œgoodâ€ word (for demo)
    return "good" in input_dict["review"].lower()

# 4) Build conditional branch
conditional = RunnableBranch(
    (is_positive, positive_chain),   # if condition true â†’ positive_chain
    negative_chain                   # else â†’ negative_chain (default)
)

# 5) Run
result = conditional.invoke({"review": "The product is good and I loved it!"})
print(result)  # Will run positive_chain

result2 = conditional.invoke({"review": "Iâ€™m disappointed with the product quality."})
print(result2)  # Will run negative_chain

```

In this example:

* If the review text contains â€œgoodâ€ â†’ route to `positive_chain`
* Otherwise â†’ route to `negative_chain`

---

### ğŸ” 5. Use-Cases

| Use-Case                 | Why Conditional Chain?                                                         |
| ------------------------ | ------------------------------------------------------------------------------ |
| Sentiment-based response | Different replies for positive vs negative feedback                            |
| Topic routing            | Route queries about â€œbillingâ€, â€œtechâ€, â€œgeneralâ€ to separate chains      |
| Input length check       | If text is very long â†’ summarise; else â†’ detailed answer                     |
| Data quality check       | If input missing fields â†’ ask user for more info; else â†’ proceed normal flow |

---

### ğŸ§© 6. Implementation Details & Tips

* Your **decision logic** can be a simple Python function (`lambda`), or even another LLM chain that classifies.
* There should be a **default branch** to handle â€œnone of the conditions matchedâ€ scenarios.
* Branches should ideally return the same schema of output (or you should normalize afterwards).
* Always test with inputs that go down each branch.
* Think about  **fallbacks** : what happens if decision logic fails?

---

### ğŸš€ 7. Real-World Analogy

Imagine youâ€™re at a restaurant kiosk:

* You press â€œVegetarianâ€ â†’ kiosk shows vegetarian menu (Chain A)
* You press â€œNon-Vegetarianâ€ â†’ shows non-veg menu (Chain B)
* If you press nothing / unclear â†’ shows â€œPlease select optionâ€ screen (Default chain)
  That is the idea of a **conditional chain** in a GenAI workflow.

---



### ğŸ“š 8. Official & Readable Resources ğŸ”—

Here are the  **best sources to learn Conditional Chains** :

1. **LangChain Docs â€“ RunnableBranch (Conditional Logic):**
   ğŸ”— [https://python.langchain.com/docs/expression_language/how_to/branching/](https://python.langchain.com/docs/expression_language/how_to/branching/)
2. **LangChain Docs â€“ Chains Overview:**
   ğŸ”— [https://python.langchain.com/docs/modules/chains/](https://python.langchain.com/docs/modules/chains/)
3. **Runnable Interface Reference (Advanced Routing):**
   ğŸ”— [https://python.langchain.com/docs/expression_language/interface/](https://python.langchain.com/docs/expression_language/interface/)

---

### âœ… 8. Summary

* **Conditional Chain** = branching logic in your chain workflow
* It allows you to dynamically choose which sub-chain to run based on input or intermediate result
* Great for real-world apps where one path doesnâ€™t always fit all inputs
* Key parts: decision function, branches (chain A, chain B, â€¦), default fallback
